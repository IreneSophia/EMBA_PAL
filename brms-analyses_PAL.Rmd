---
title: "S1: behavioural, model-agnostic analysis with brms"
author: "I. S. Plank"
date: "`r Sys.Date()`"
output: pdf_document
---
  
```{r settings, include=FALSE}

knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.align = 'center', fig.width = 9)
ls.packages = c("knitr",# kable
    "ggplot2",          # plots
    "brms",             # Bayesian lmms
    "designr",          # simLMM
    "bridgesampling",   # bridge_sampler
    "tidyverse",        # tibble stuff
    "ggpubr",           # ggarrange
    "ggrain",           # geom_rain
    "bayesplot",        # plots for posterior predictive checks
    "SBC",              # plots for checking computational faithfulness
    "rstatix",          # anova
    "BayesFactor", 
    "bayestestR"        # equivalence_test
)

lapply(ls.packages, library, character.only=TRUE)

# set cores
options(mc.cores = parallel::detectCores())

# set options for SBC
use_cmdstanr = getOption("SBC.vignettes_cmdstanr", TRUE) # Set to false to use rstan instead
options(brms.backend = "cmdstanr")

# using parallel processing
library(future)
plan(multisession)

# Setup caching of results
cache_dir = "./_brms_SBC_cache"
if(!dir.exists(cache_dir)) {
  dir.create(cache_dir)
}

# load the function to perform the sensitivity analysis
source('helper/fun_bf-sens.R')

# graph settings 
c_light = "#a9afb2"; c_light_highlight = "#8ea5b2"; c_mid = "#6b98b2" 
c_mid_highlight = "#3585b2"; c_dark = "#0072b2"; c_dark_highlight = "#0058b2" 
c_green = "#009E73"
sz = 1
a = 0.5

# custom colour palette
custom.col = c("#009E73", "#D55E00", c_dark_highlight, "#CC79A7")

# calculate visual angle: px.x should be a vector of c(width, height) in pixel
vis_ang = function(px.x) {
  
  # infos on the setup
  mm.w = 344    # monitor width in mm
  mm.h = 215    # monitor height in mm
  px.w = 2650   # monitor resolution: width
  px.h = 1600   # monitor resolution: height
  dist = 570    # viewing distance
  
  # convert width from screen pixel size to mm and take half
  mm.x    = (px.x[1] / (px.w/mm.w))/2
  
  # convert height from screen pixel size to mm and take half
  mm.x[2] = (px.x[2] / (px.h/mm.h))/2
  
  # calculate angles
  rad.alpha    = atan(mm.x[1]/dist)
  rad.alpha[2] = atan(mm.x[2]/dist)
  
  # convert to degrees and double again
  deg.alpha    = (rad.alpha[1] / (pi/180)) * 2
  deg.alpha[2] = (rad.alpha[2] / (pi/180)) * 2
  
  return(deg.alpha)
  
}

```

<style type="text/css">
  .main-container {
    max-width: 1100px;
    margin-left: auto;
    margin-right: auto;
  }
</style>
  
# S1.1 Introduction
  
This R Markdown script analyses data from the PAL (probabilistic associative learning) task of the EMBA project. The data was preprocessed before being read into this script. 

In this task, participants view faces for which they have to judge whether the portrayed emotion is positive or negative. The faces are preceeded by a tone which is predictive of the valence of the following face. The visual angle of the faces was `r round(vis_ang(c(265,350))[1],2)` degrees wide and `r round(vis_ang(c(265,350))[2],2)` degrees high.

## Some general settings

```{r set}

# number of simulations
nsim = 500

# set number of iterations and warmup for models
iter = 6000
warm = 1500

# set the seed
set.seed(2468)

```

## Package versions

The following packages are used in this RMarkdown file: 
  
```{r lib_versions, echo=F}

print(R.Version()$version.string)

for (package in ls.packages) {
  print(sprintf("%s version %s", package, packageVersion(package)))
}

```

## Introduction

We planned to determine the group-level effect subjects following Barr (2013). For each model, experiment specific priors were set based on previous literature or the task (see comments in the code).

We perform prior predictive checks as proposed in Schad, Betancourt and Vasishth (2020) using the SBC package. To do so, we create `r nsim` simulated datasets where parameters are simulated from the priors. These parameters are used to create one fake dataset. Both the true underlying parameters and the simulated discrimination values are saved. 

Then, we create graphs showing the prior predictive distribution of the simulated discrimination threshold to check whether our priors fit our general expectations about the data. Next, we perform checks of computational faithfulness and model sensitivity as proposed by Schad, Betancourt and Vasishth (2020) and implemented in the SBC package. We create models for each of the simulated datasets. Last, we calculate performance metrics for each of these models, focusing on the population-level parameters. 

## Preparation

First, we load the data and combine it with demographic information including the diagnostic status of the subjects. Then, all predictors are set to sum contrasts. 

```{r prep}


# check if the data file exists, if yes load it:
if (!file.exists("PAL_data.RData")) {

  # get demo info for subjects
  df.sub = read_csv(file.path("/home/emba/Documents/EMBA/CentraXX", "EMBA_centraXX.csv"), 
                    show_col_types = F) %>%
    mutate(
      diagnosis = as.factor(recode(diagnosis, "CTR" = "COMP"))
    )
  
  # set the data path
  dt.path  = "/home/emba/Documents/EMBA/BVET"
  dt.explo = "/home/emba/Documents/EMBA/BVET-explo"
  
  # load the preprocessed data: df.tsk
  df.tsk = rbind(readRDS(file.path(dt.path,  "PAL_tsk.rds")),
                 readRDS(file.path(dt.explo, "PAL_tsk.rds")))
  
  # load excluded participants (accuracy < 2/3)
  exc = c(scan(file.path(dt.path, 'PAL_exc.txt'),  what="character", sep=NULL),
          scan(file.path(dt.explo, 'PAL_exc.txt'), what="character", sep=NULL))
  df.exc = df.sub %>% filter(subID %in% exc) %>% 
    select(diagnosis) %>% 
    group_by(diagnosis) %>% count()
  
  # merge with group
  df.tsk = merge(df.sub %>% select(subID, diagnosis), 
                 df.tsk, all.y = T) %>%
    mutate_if(is.character, as.factor)
  
  # load data and aggregate emotion discrimination threshold
  df.fer = rbind(readRDS(file = paste0(dt.path, '/df_FER.RDS')),
                 readRDS(file = paste0(dt.explo, '/df_FER.RDS')))%>%
    group_by(subID, emo) %>% 
    summarise(
      EDT = mean(disc, na.rm = T)
    ) %>% 
    group_by(subID) %>%
    summarise(
      EDT = mean(EDT)
    )
  
  # only keep participants included in the study in the subject data frame
  df.sub = merge(df.tsk %>% select(subID) %>% distinct(), df.sub, all.x = T) %>%
    # merge with EDT dataframe
    merge(., df.fer, all.x = T)
  
  # [!MISSING: load the eye tracking data]

  # anonymise the data
  df.tsk = df.tsk %>%
    mutate(
      PID = subID,
      subID = as.factor(as.numeric(subID))
    )
  
  # get a correspondence of original PIDs and anonymised subIDs
  df.recode = df.tsk %>% select(PID, subID) %>% distinct()
  recode = as.character(df.recode$subID)
  names(recode) = df.recode$PID
  df.tsk = df.tsk %>% select(-PID)
  
  # anonymise other data in the same way [!MISSING]
  #df.et$subID = str_replace_all(df.var$subID, recode)
  
  # print gender frequencies and compare them across groups
  tb.gen = xtabs(~ gender + diagnosis, data = df.sub)
  ct.full = contingencyTableBF(tb.gen, 
                               sampleType = "indepMulti", 
                               fixedMargin = "cols")
  # since only DAN in the ADHD group, we try again after excluding them
  ct.mf = contingencyTableBF(tb.gen[2:3,], 
                             sampleType = "indepMulti", 
                             fixedMargin = "cols")
  tb.gen = xtabs(~ gender + diagnosis + cis, data = df.sub)
  
  # get the gender descriptions of the not-male and not-female participants
  gen.desc = unique(tolower(df.sub[df.sub$gender == "dan",]$gender_desc))
  
  # check which outcomes of interest are normally distributed
  df.sht = df.sub %>% 
    group_by(diagnosis) %>%
    shapiro_test(age, iq, BDI_total, ASRS_total, RAADS_total, TAS_total) %>%
    mutate(
      sig = if_else(p < 0.05, "*", "")
    ) %>% arrange(variable)
  
  # most of the measures are not normally distributed;
  # therefore, we compute ranks for these outcomes
  df.sub = df.sub %>% 
    ungroup() %>% 
    mutate(
      rASRS  = rank(ASRS_total),
      rage   = rank(age),
      rBDI   = rank(BDI_total),
      rRAADS = rank(RAADS_total),
      rTAS   = rank(TAS_total),
      diagnosis = as.factor(diagnosis)
    )
  
  # now we can compute our ANOVAs
  aov.age   = anovaBF(rage   ~ diagnosis, data = df.sub)
  aov.iq    = anovaBF(iq     ~ diagnosis, data = df.sub)
  aov.BDI   = anovaBF(rBDI   ~ diagnosis, data = df.sub)
  aov.ASRS  = anovaBF(rASRS  ~ diagnosis, data = df.sub)
  aov.RAADS = anovaBF(rRAADS ~ diagnosis, data = df.sub)
  aov.TAS   = anovaBF(rTAS   ~ diagnosis, data = df.sub)
  
  # ...and put everything in a new dataframe for printing
  measurement  = "Age"
  ADHD     = sprintf("%.2f (±%.2f)", 
                     mean(df.sub[df.sub$diagnosis == "ADHD",]$age), 
                     sd(df.sub[df.sub$diagnosis == "ADHD",]$age)/
                       sqrt(nrow(df.sub[df.sub$diagnosis == "ADHD",])))
  ASD      = sprintf("%.2f (±%.2f)", 
                     mean(df.sub[df.sub$diagnosis == "ASD",]$age), 
                     sd(df.sub[df.sub$diagnosis == "ASD",]$age)/
                       sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",])))
  `ADHD+ASD`     = sprintf("%.2f (±%.2f)", 
                     mean(df.sub[df.sub$diagnosis == "BOTH",]$age), 
                     sd(df.sub[df.sub$diagnosis == "BOTH",]$age)/
                       sqrt(nrow(df.sub[df.sub$diagnosis == "BOTH",])))
  COMP     = sprintf("%.2f (±%.2f)", 
                     mean(df.sub[df.sub$diagnosis == "COMP",]$age), 
                     sd(df.sub[df.sub$diagnosis == "COMP",]$age)/
                       sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",])))
  logBF10 = sprintf("%.3f", aov.age@bayesFactor[["bf"]])
  df.table = data.frame(measurement, ADHD, ASD, `ADHD+ASD`, COMP, logBF10)
  df.table = rbind(df.table,
       c(
         "ASRS",
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "ADHD",]$ASRS_total), 
                 sd(df.sub[df.sub$diagnosis == "ADHD",]$ASRS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "ADHD",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "ASD",]$ASRS_total), 
                 sd(df.sub[df.sub$diagnosis == "ASD",]$ASRS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "BOTH",]$ASRS_total), 
                 sd(df.sub[df.sub$diagnosis == "BOTH",]$ASRS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "BOTH",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "COMP",]$ASRS_total), 
                 sd(df.sub[df.sub$diagnosis == "COMP",]$ASRS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
         sprintf("%.3f", aov.ASRS@bayesFactor[["bf"]])
       ),
       c(
         "BDI",
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "ADHD",]$BDI_total), 
                 sd(df.sub[df.sub$diagnosis == "ADHD",]$BDI_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "ADHD",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "ASD",]$BDI_total), 
                 sd(df.sub[df.sub$diagnosis == "ASD",]$BDI_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "BOTH",]$BDI_total), 
                 sd(df.sub[df.sub$diagnosis == "BOTH",]$BDI_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "BOTH",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "COMP",]$BDI_total), 
                 sd(df.sub[df.sub$diagnosis == "COMP",]$BDI_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
         sprintf("%.3f", aov.BDI@bayesFactor[["bf"]])
       ),
       c(
         "Gender (diverse/agender/non-binary - female - male)",
         sprintf("%d - %d - %d", 
                 nrow(df.sub[df.sub$diagnosis == "ADHD" & df.sub$gender == "dan",]), 
                 nrow(df.sub[df.sub$diagnosis == "ADHD" & df.sub$gender == "fem",]), 
                 nrow(df.sub[df.sub$diagnosis == "ADHD" & df.sub$gender == "mal",])),
         sprintf("%d - %d - %d", 
                 nrow(df.sub[df.sub$diagnosis == "ASD" & df.sub$gender == "dan",]), 
                 nrow(df.sub[df.sub$diagnosis == "ASD" & df.sub$gender == "fem",]), 
                 nrow(df.sub[df.sub$diagnosis == "ASD" & df.sub$gender == "mal",])),
         sprintf("%d - %d - %d", 
                 nrow(df.sub[df.sub$diagnosis == "BOTH" & df.sub$gender == "dan",]), 
                 nrow(df.sub[df.sub$diagnosis == "BOTH" & df.sub$gender == "fem",]), 
                 nrow(df.sub[df.sub$diagnosis == "BOTH" & df.sub$gender == "mal",])),
         sprintf("%d - %d - %d", 
                 nrow(df.sub[df.sub$diagnosis == "COMP" & df.sub$gender == "dan",]), 
                 nrow(df.sub[df.sub$diagnosis == "COMP" & df.sub$gender == "fem",]), 
                 nrow(df.sub[df.sub$diagnosis == "COMP" & df.sub$gender == "mal",])),
         sprintf("%.3f", ct.full@bayesFactor[["bf"]])
       ),
       c(
         "IQ",
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "ADHD",]$iq), 
                 sd(df.sub[df.sub$diagnosis == "ADHD",]$iq)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "ADHD",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "ASD",]$iq), 
                 sd(df.sub[df.sub$diagnosis == "ASD",]$iq)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "BOTH",]$iq), 
                 sd(df.sub[df.sub$diagnosis == "BOTH",]$iq)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "BOTH",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "COMP",]$iq), 
                 sd(df.sub[df.sub$diagnosis == "COMP",]$iq)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
         sprintf("%.3f", aov.iq@bayesFactor[["bf"]])
       ),
       c(
         "RAADS",
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "ADHD",]$RAADS_total), 
                 sd(df.sub[df.sub$diagnosis == "ADHD",]$RAADS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "ADHD",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "ASD",]$RAADS_total), 
                 sd(df.sub[df.sub$diagnosis == "ASD",]$RAADS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "BOTH",]$RAADS_total), 
                 sd(df.sub[df.sub$diagnosis == "BOTH",]$RAADS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "BOTH",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "COMP",]$RAADS_total), 
                 sd(df.sub[df.sub$diagnosis == "COMP",]$RAADS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
         sprintf("%.3f", aov.RAADS@bayesFactor[["bf"]])
       ),
       c(
         "TAS",
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "ADHD",]$TAS_total), 
                 sd(df.sub[df.sub$diagnosis == "ADHD",]$TAS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "ADHD",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "ASD",]$TAS_total), 
                 sd(df.sub[df.sub$diagnosis == "ASD",]$TAS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "BOTH",]$TAS_total), 
                 sd(df.sub[df.sub$diagnosis == "BOTH",]$TAS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "BOTH",]))),
         sprintf("%.2f (±%.2f)", 
                 mean(df.sub[df.sub$diagnosis == "COMP",]$TAS_total), 
                 sd(df.sub[df.sub$diagnosis == "COMP",]$TAS_total)/
                   sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
         sprintf("%.3f", aov.TAS@bayesFactor[["bf"]])
       )
  ) %>% arrange(measurement)
  
  # we aggregate the correct and the usable reaction times due to suboptimal
  # posterior predictive fit of the full model
  df.pal = df.tsk %>%
    group_by(subID, diagnosis, phase, expected, difficulty) %>%
    summarise(
      rt.cor = median(rt.cor, na.rm = T),
      rt.use = median(rt.use, na.rm = T),
      acc    = 100 * mean(acc) # accuracy in percent
    )
  
  # calculate variance of reaction times: 
  # no difficulty due to suboptimal posterior fit
  df.var = df.tsk %>%
    group_by(subID, diagnosis, phase, expected) %>% 
    summarise(
      totaln = n(),
      valuen = sum(!is.na(rt.cor)),
      rt.var = sd(rt.cor, na.rm = T)
    ) %>%
    mutate(
      perc = valuen/totaln
    ) %>% filter(perc >= 2/3) %>%
    mutate_if(is.character, as.factor)
  
  # we also aggregate it to check whether there were any differences between 
  # tone_pic combinations: 
  df.tp = df.tsk %>%
    filter(expected != "neutral") %>% droplevels() %>%
    mutate(
      tone_pic = if_else(ut == 1, "highpos_lowneg", "highneg_lowpos")
    ) %>%
    group_by(subID, diagnosis, tone_pic, expected) %>%
    summarise(
      rt.cor = median(rt.cor, na.rm = T)
    )
  
  # save it all
  save(df.pal, df.var, df.tp, 
       df.table, df.sht, ct.full, ct.mf, df.exc, gen.desc, tb.gen, 
       file = "PAL_data.RData")
  
} else {
  
  load("PAL_data.RData")
  
}

# print the group of excluded participants
kable(df.exc)
rm(df.exc)

# print the outcome of the shapiro tests
kable(df.sht)
rm(df.sht)

# print the outcome of the two contingency tables for comparison: all participants
ct.full@bayesFactor
# only male and female participants
ct.mf@bayesFactor
# print descriptions in not-female and not-male group > remove umlaute
print(gsub("[[:punct:]]", "", gen.desc))
# print gender and trans/cis distribution
kable(tb.gen)

# drop the neutral condition for the analysis
df.pal = df.pal %>%
  filter(expected != "neutral") %>% droplevels() %>%
  mutate_if(is.character, as.factor) %>%
  ungroup()
df.var = df.var %>%
  filter(expected != "neutral") %>% droplevels() %>%
  mutate_if(is.character, as.factor) %>%
  ungroup()

# set and print the contrasts
contrasts(df.pal$diagnosis) = contr.sum(4)
contrasts(df.pal$diagnosis)
contrasts(df.pal$expected) = contr.sum(2)
contrasts(df.pal$expected)
contrasts(df.pal$phase) = contr.sum(3)
contrasts(df.pal$phase)
contrasts(df.pal$difficulty) = contr.sum(3)
contrasts(df.pal$difficulty)

contrasts(df.var$diagnosis) = contr.sum(4)
contrasts(df.var$diagnosis)
contrasts(df.var$expected) = contr.sum(2)
contrasts(df.var$expected)
contrasts(df.var$phase) = contr.sum(3)
contrasts(df.var$phase)

# print final group comparisons for the paper
kable(df.table)

```

The three diagnostic groups are similar in age, IQ and gender distribution. However, they seem to differ in their questionnaire scores measuring ADHD (ASRS), depression (BDI), autism (RAADS) and alexithymia (TAS). 

# S1.2 Reaction time variance

In the preregistration, we noted the following population-level effects for the model of the reaction time variances: group, expectancy, phase and difficulty. However, the posterior fit for this model was suboptimal, therefore, we dropped the predictor difficulty. 

## Model setup

```{r model_var}

# figure out slopes for subjects
kable(head(df.var %>% count(subID, expected)))
kable(head(df.var %>% count(subID, phase)))
kable(head(df.var %>% count(subID, expected, phase)))

# code for filenames
code = "PAL-var"

# model formula
f.var = brms::bf(rt.var ~ diagnosis * expected * phase +
                   (expected + phase | subID)
                 )

# set weakly informative priors
priors = c(
  prior(normal(4.50, 0.50),  class = Intercept),
  prior(normal(0,    0.50),  class = sigma),
  prior(normal(0.15, 0.15),  class = sd),
  prior(normal(0,    0.25),  class = b)
)

```

## Simulation-based calibration

```{r sbc_var1}

# check if the SBC already exists
if (file.exists(file.path(cache_dir, sprintf("df_res_%s.rds", code)))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, sprintf("df_res_%s.rds", code)))
  df.backend = readRDS(file.path(cache_dir, sprintf("df_div_%s.rds", code)))
} else {
  # stimulate some data
  set.seed(2486)
  gen = SBC_generator_brms(f.var, data = df.var, prior = priors, 
   thin = 50, warmup = 10000, refresh = 2000,
   generate_lp = TRUE, family = lognormal, init = 0.1)
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
    warmup = warm, iter = iter)
  dat = generate_datasets(gen, nsim)
  saveRDS(dat, file.path(cache_dir, sprintf("dat_%s.rds", code)))
  # perform the SBC on the first 250 simulated datasets
  res = compute_SBC(SBC_datasets(dat$variables[1:250,],
                        dat$generated[1:250]), 
        bck,
        cache_mode     = "results", 
        cache_location = file.path(cache_dir, sprintf("res_%s", code)))
  saveRDS(res$stats, file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(res$backend_diagnostics, file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
}

```

We start by investigating the rhats and the number of divergent samples. This shows that only `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05 and `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples. This suggests that this model performs well. 

Next, we can plot the simulated values to perform prior predictive checks. 

```{r sbc_var2, fig.height=8}

if (!(file.exists(file.path(cache_dir, sprintf("dvfakemat_%s", code))))) {
  # create a matrix out of generated data
  dvname = gsub(" ", "", gsub("[\\|~].*", "", f.var)[1])
  dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
  for (i in 1:length(dat[['generated']])) {
    dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
  }
  saveRDS(dvfakemat, file.path(cache_dir, sprintf("dvfakemat_%s", code)))
} else {
  dvfakemat = readRDS(file.path(cache_dir, sprintf("dvfakemat_%s", code)))
}

# compute one histogram per simulated data-set 
dvfakematH = dvfakemat
dvfakematH[dvfakematH > 1000] = 1000
breaks = seq(0, max(dvfakematH, na.rm=T), length.out = 101) 
binwidth = breaks[2] - breaks[1]
histmat = matrix(NA, ncol = dim(dvfakemat)[2], nrow = length(breaks)-1) 
for (i in 1:dim(dvfakemat)[2]) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat = as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Distribution of simulated reaction time variances", y = "", x = "") +
  theme_bw()

tmpM = apply(dvfakemat, 2, mean) # mean 
tmpSD = apply(dvfakemat, 2, sd) 
p2 = ggplot() + 
  stat_bin(aes(x = tmpM), fill = c_dark)  + 
  labs(x = "Mean discrimination", title = "Means of simulated reaction time variances") +
  theme_bw()
p3 = ggplot() + 
  stat_bin(aes(x = tmpSD), fill = c_dark) + 
  labs(x = "SD discrimination", title = "SDs of simulated reaction time variances") +
  theme_bw()
p = ggarrange(p1, 
  ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
  nrow = 2, labels = "A")
annotate_figure(p, top = text_grob("Prior predictive checks", face = "bold", size = 14))

```

Subfigure B shows the distribution of the simulated data with bluer bands being more likely than greyer bands. It shows a general distribution that fits our expectations, even though there are quite a few values that are unreastically large. This is because we had to increase the standard deviations in the priors to achieve appropriate contraction values. The same applies to the distribution of the means and standard deviations in the simulated datasets. We go ahead with these priors and check the results of the SBC. We only plot the results from the models that had no divergence issues. 

```{r sbc_var3, fig.height=9}

# get simulation numbers with issues
mx_rnk = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(rhat = max(rhat, na.rm = T),
                max_rank = mean(max_rank)) %>% 
    filter(rhat >= 1.05 | max_rank != mx_rnk), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters
a = 0.5
df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id))
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, labels = "AUTO", ncol = 1, nrow = 2)
annotate_figure(p, top = text_grob("Computational faithfulness and model sensitivity: reaction time variance", face = "bold", size = 14))

```

Next, we check the ranks of the parameters. If the model is unbiased, these should be uniformly distributed (Schad, Betancourt and Vasishth, 2020). The sample empirical cumulative distribution function (ECDF) shows deviations from the theoretical distribution (95%) and the rank histogram also shows ranks outside the 95% expected range. However, we judge this to be acceptable as we cannot identify clear bias patterns as described in the information accompanying the SBC package: https://hyunjimoon.github.io/SBC/articles/rank_visualizations.html

```{r sbc_var4, fig.height=9}

p3 = plot_sim_estimated(df.results.b, alpha = .8) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(df.results.b, prior_sd = setNames(c(0.50, rep(0.25, length(unique(df.results.b$variable))-1)), unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p3, p4, labels = "AUTO", ncol = 1, nrow = 2)
annotate_figure(p, top = text_grob("Computational faithfulness and model sensitivity: reaction time variance", face = "bold", size = 14))

```

Then, we investigated the relationship between the simulated true parameters and the posterior estimates. Although there are individual values diverging from the expected pattern, most parameters were recovered successfully within an uncertainty interval of alpha = 0.05. 

Last, we explore the z-score and the posterior contraction of our population-level predictors. The z-score "determines the distance of the posterior mean from the true simulating parameter", while the posterior contraction "estimates how much prior uncertainty is reduced in the posterior estimation" (Schad, Betancourt and Vasisth, 2020). The contraction reveals very narrow posterior standard deviations with slightly lower contraction scores than we would want in the optimal case, however, we judge this as acceptable. 

## Posterior predictive checks

As the next step, we fit the model, check whether there are divergence or rhat issues, and then check whether the chains have converged.

```{r postpc_var1, fig.height=9, message=T}

# fit the final model
set.seed(2684)
m.var = brm(f.var,
            df.var, prior = priors,
            family = lognormal,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_var",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.var$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.var) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.var)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 4)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no pathological behaviour with E-BFMI, no divergent sample and no rhats that are higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc_var2, fig.height=9}

# get posterior predictions
post.pred = posterior_predict(m.var, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.var, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none") + xlim(0, 1000)

# distributions of means compared to the real values per group
p2 = ppc_stat_grouped(df.var$rt.var, post.pred, df.var$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means compared to the real values per group
p2 = ppc_stat_grouped(df.var$rt.var, post.pred, df.var$diagnosis) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, 
          nrow = 2, ncol = 1, labels = "AUTO")
annotate_figure(p, top = text_grob("Posterior predictive checks: RT variance", face = "bold", size = 14))

```

The simulated data based on the model fits well with the real data. 

## Inferences

Now that we are convinced that we can trust our model, we have a look at its estimate and use the hypothesis function to assess our hypotheses and perform explorative tests. 

```{r inf_var, fig.height=9}

# print a summary
summary(m.var)

# get the estimates and compute group comparisons
df.m.var = post.draws %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP         = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3,
    b_postvolatile = - b_phase1 - b_phase2,
    ASD            = b_Intercept + b_diagnosis2,
    BOTH           = b_Intercept + b_diagnosis3,
    ADHD           = b_Intercept + b_diagnosis1,
    COMP           = b_Intercept + b_COMP,
    `ADHD-ASD`     = ADHD - ASD,
    `ADHD-COMP`    = ADHD - COMP,
    `ASD-COMP`     = ASD  - COMP
  )

# plot the posterior distributions
df.m.var %>%
  select(starts_with("b_")) %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  subset(!startsWith(coef, "b_Int")) %>%
  mutate(
    coef = substr(coef, 3, nchar(coef)),
    coef = str_replace_all(coef, ":", " x "),
    coef = str_replace_all(coef, "diagnosis1", "ADHD"),
    coef = str_replace_all(coef, "diagnosis2", "ASD"),
    coef = str_replace_all(coef, "diagnosis3", "ADHD+ASD"),
    coef = str_replace_all(coef, "expected1", "expected"),
    coef = str_replace_all(coef, "expected2", "unexpected"),
    coef = str_replace_all(coef, "phase1", "prevolatile"),
    coef = str_replace_all(coef, "phase2", "volatile"),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(c_dark, c_light)) + theme(legend.position = "none")

# H1a: COMP < ADHD
h1a = hypothesis(m.var, "0 < 2*diagnosis1 + diagnosis2")
h1a

## exploration

# expected versus unexpected
e1 = hypothesis(m.var, "0 < expected1", alpha = 0.025)
e1

# volatile versus prevolatile
e2 = hypothesis(m.var, "0 < phase1 + 2*phase2", alpha = 0.025)
e2

# prevolatile versus postvolatile
e3 = hypothesis(m.var, "0 < 2*phase1 + phase2", alpha = 0.025)
e3

# volatile versus postvolatile
e4 = hypothesis(m.var, "0 > phase1 - phase2", alpha = 0.025)
e4

# ADHD versus ASD
e5 = hypothesis(m.var, "0 < diagnosis1 - diagnosis2", alpha = 0.025)
e5

# ASD versus COMP
e6 = hypothesis(m.var, "0 < diagnosis1 + 2*diagnosis2", alpha = 0.025)
e6

# equivalence of these groups
e1_equ = equivalence_test(df.m.var %>% select(`ADHD-COMP`, `ASD-COMP`, `ADHD-ASD`))
e1_equ

## extract predicted differences in ms instead of log data
df.new = df.var %>% 
  select(diagnosis, phase, expected) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, phase, expected, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.var, summary = F, 
               newdata = df.new %>% select(diagnosis, phase, expected), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

# calculate our difference columns
df.ms = df.ms %>%
  mutate(
    COMP          = rowMeans(across(matches("COMP_.*"))),
    ADHD          = rowMeans(across(matches("ADHD_.*"))),
    ASD           = rowMeans(across(matches("ASD_.*"))),
    h1a_COMPvADHD = ADHD - COMP,
    exp           = rowMeans(across(matches(".*_expected"))),
    unexp         = rowMeans(across(matches(".*_unexpected"))),
    e1_expvunexp  = exp - unexp,
    volatile      = rowMeans(across(matches(".*_volatile_.*"))),
    prevolatile   = rowMeans(across(matches(".*_prevolatile_.*"))),
    postvolatile  = rowMeans(across(matches(".*_postvolatile_.*"))),
    e2_volvpre    = volatile - prevolatile,
    e3_postvpre   = postvolatile - prevolatile,
    e4_volvpost   = volatile - postvolatile,
    e5_ADHDvASD   = ADHD - ASD,
    e6_COMPvASD   = COMP - ASD
  )

lapply(df.ms %>% select(starts_with("e") | starts_with("h")), ci)

```

[!MISSING]

## Plots

```{r plot_var, fig.height=6}

# rain cloud plot

df.var %>%
  ggplot(aes(expected, rt.var, fill = diagnosis, colour = diagnosis)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show.legend = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  #ylim(0, 1) +
  labs(title = "Reaction times variance per subject", x = "", y = "variance") +
  facet_wrap(. ~ phase) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5), legend.direction = "horizontal", text = element_text(size = 15))

```

Now, we use line plots to visualise out the different main effects and interaction effects in our data. 

```{r plot_var2, fig.height=9}

# two-way interactions
p1 = df.var %>% 
  group_by(diagnosis, expected) %>%
  summarise(
    var.mn = mean(rt.var),
    var.se = sd(rt.var) / sqrt(n())
  ) %>%
  ggplot(aes(y = var.mn, x = expected, group = diagnosis, colour = diagnosis)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = var.mn - var.se, 
                    ymax = var.mn + var.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "expectancy", y = "rt var (ms)") + 
  ggtitle("Diagnosis x expectancy") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))
p2 = df.var %>% 
  group_by(diagnosis, phase) %>%
  summarise(
    var.mn = mean(rt.var),
    var.se = sd(rt.var) / sqrt(n())
  ) %>%
  ggplot(aes(y = var.mn, x = phase, group = diagnosis, colour = diagnosis)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = var.mn - var.se, 
                    ymax = var.mn + var.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "phase", y = "rt var (ms)") + 
  ggtitle("Diagnosis x phase") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))
p3 = df.var %>% 
  group_by(phase, expected) %>%
  summarise(
    var.mn = mean(rt.var),
    var.se = sd(rt.var) / sqrt(n())
  ) %>%
  ggplot(aes(y = var.mn, x = expected, group = phase, colour = phase)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = var.mn - var.se, 
                    ymax = var.mn + var.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "expectancy", y = "rt var (ms)") + 
  ggtitle("Phase x expectancy") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))

# three-way interaction
p4 =  df.var %>% 
  group_by(diagnosis, expected, phase) %>%
  summarise(
    var.mn = mean(rt.var),
    var.se = sd(rt.var) / sqrt(n())
  ) %>%
  ggplot(aes(y = var.mn, x = expected, group = diagnosis, colour = diagnosis)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = var.mn - var.se, 
                    ymax = var.mn + var.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "expectancy", y = "rt var (ms)") + 
  ggtitle("Diagnosis x phase x expectancy") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  facet_wrap(. ~ phase) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))

ggarrange(p1, p2, p3, p4, ncol = 2, nrow = 2, labels = "AUTO")

```

## Bayes factor analysis

To complement our hypothesis testing using brms::hypothesis(), we perform a Bayes Factor (BF) analysis. The BF is the ratio of the marginal likelihoods of the data given two models. We will compare models containing different combinations of population-level effects to the model only containing the intercept on the population-level and all group-level effects. The BF depends on the priors that were used, because it indicates a change in our belief after seeing the data. Therefore, we perform a sensitivity analysis comparing the BF based on our chosen priors with narrower and wider priors. 

```{r bf_var, fig.height=4}

# set the directory in which to save results
sense_dir = file.path(getwd(), "_brms_sens_cache")
main.code = "pal_var"

# rerun the model with more iterations for bridgesampling
set.seed(2684)
m.var.bf = brm(f.var,
            df.var, prior = priors,
            family = lognormal,
            iter = 40000, warmup = 10000,
            backend = "cmdstanr", threads = threading(8),
            file = "m_var_bf",
            save_pars = save_pars(all = TRUE)
            )

# describe priors for the sensitivity analysis
pr.descriptions = c("chosen",
  "sdx2",    "sdx4",   "sdx8", 
  "sdx0.5", "sdx0.25", "sdx0.125"
  )

# check which have been run already
if (file.exists(file.path(sense_dir, sprintf("df_%s_bf.csv", main.code)))) {
  pr.done = read_csv(
    file.path(sense_dir, sprintf("df_%s_bf.csv", main.code)), 
    show_col_types = F) %>%
    select(priors) %>% distinct()
  pr.descriptions = pr.descriptions[!(pr.descriptions %in% pr.done$priors)]
}

# loop through the priors that have not been used before
for (pr.desc in pr.descriptions) {
  tryCatch({
    # use function to compute BF with the described priors
    bf_sens_3int(m.var.bf, "diagnosis", "phase", "expected", 
                 pr.desc, 
                 main.code, # prefix for all models and MLL
                 file.path("/home/emba/Insync/10planki@gmail.com/Google Drive/NEVIA/logfiles", "log_PAL-var_bf.txt"), # log file
                 sense_dir # where to save the models and MLL
    )
  },
  error = function(err) {
    message(sprintf("Error for %s: %s", pr.desc, err))
  }
  )
}

df.var.bf = read_csv(file.path(sense_dir,
                               sprintf("df_%s_bf.csv", main.code)),
                     show_col_types = F)

# check the sensitivity analysis result per model
df.var.bf %>%
  filter(`population-level` != "1") %>%
  mutate(
    sd = as.factor(case_when(
      priors == "chosen" ~ "1",
      substr(priors, 1, 3) == "sdx" ~ gsub("sdx", "", priors),
      T ~ priors)
    ),
    order = case_when(
      priors == "chosen" ~ 1,
      substr(priors, 1, 3) == "sdx" ~ as.numeric(gsub("sdx", "", priors)),
      T ~ 999),
    sd = fct_reorder(sd, order)
  ) %>%
  ggplot(aes(y = bf.log,
             x = sd,
             group = `population-level`,
             colour = `population-level`)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = "1") +
  geom_hline(yintercept = 0) +
  ggtitle("Sensitivity analysis with the intercept-only model as reference") +
  #scale_colour_manual(values = custom.col) +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# create a data frame with the comparisons
kable(head(df.var.bf %>% filter(priors == "chosen") %>% select(-priors) %>%
  filter(`population-level` != "1") %>% arrange(desc(bf.log))), digits = 3)

```

# S1.3 Reaction times in correct trials

In the preregistration, we noted the following population-level effects for the model of the reaction time variances: group, expectancy, phase and difficulty; as well as the group-level predictores subject and trials. However, the posterior fit for the full model was suboptimal, therefore, we aggregated the reaction times for correct trials using the median (see above in the preparation of the data). 


```{r model_rt}

# figure out slopes for subjects
kable(head(df.pal %>% count(subID, expected)))
kable(head(df.pal %>% count(subID, phase)))
kable(head(df.pal %>% count(subID, difficulty)))
kable(head(df.pal %>% count(subID, expected, phase)))
kable(head(df.pal %>% count(subID, expected, difficulty)))
kable(head(df.pal %>% count(subID, phase, difficulty)))
kable(head(df.pal %>% count(subID, expected, phase, difficulty)))

code = "PAL-rt"

# set the formula
f.pal = brms::bf(rt.cor ~ diagnosis * expected * phase * difficulty +
                   (expected + phase + difficulty +
                      expected:phase + difficulty:phase + expected:difficulty | subID))

# set informed priors based on previous results
priors = c(
  # informative priors based Lawson et al. and Schad, Betancourt & Vasishth (2019)
  prior(normal(6.0,   0.3),   class = Intercept),
  prior(normal(0.0,   0.5),   class = sigma),
  prior(normal(0,     0.1),   class = sd),
  prior(lkj(2),              class = cor),
  prior(normal(100, 100.0),   class = ndt),
  # ASD slower overall (Lawson et al., 2017)
  prior(normal(0.02,  0.04),  class = b, coef = diagnosis2), 
  # faster for expected trials (Lawson et al., 2017)
  prior(normal(-0.02, 0.04),  class = b, coef = expected1), # expected
  # faster on easy trials (Lawson et al., 2017)
  prior(normal(-0.02, 0.04),  class = b, coef = difficulty1), # easy
  # larger effect of phases in ASD (Shi et al., 2022)
  prior(normal(0.02,  0.04),  class = b, coef = diagnosis2:phase2),
  prior(normal(0.02,  0.04),  class = b, coef = diagnosis2:phase1),
  # smaller effect of expected in ASD (Lawson et al., 2017)
  prior(normal(0.02,  0.04),  class = b, coef = diagnosis2:expected1), 
  # all the other interactions
  prior(normal(0.00,  0.04),  class = b)
)

```

## Simulation-based calibration

```{r sbc_rt1}

if (!(file.exists(file.path(cache_dir, sprintf("df_res_%s.rds", code)))) | 
    !(file.exists(file.path(cache_dir, sprintf("df_res_%s.rds", code))))) {
  # simulate some data based on the priors
  gen = SBC_generator_brms(f.pal, data = df.pal, prior = priors, 
                           thin = 50, warmup = 10000, refresh = 2000,
                           generate_lp = TRUE, family = shifted_lognormal, init = 0.1)
  if (!file.exists(file.path(cache_dir, paste0("dat_", code, ".rds")))){
    dat = generate_datasets(gen, nsim)
    saveRDS(dat, file = file.path(cache_dir, paste0("dat_", code, ".rds")))
  } else {
    dat = readRDS(file = file.path(cache_dir, paste0("dat_", code, ".rds")))
  }
  
  # perform the SBC
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1,
                                      warmup = warm, iter = iter,
                                      init = 0.1)
  # only do it for 250 simulations since it takes so long 
  # > this was done with the helper script, seeds are documented there!
  res = compute_SBC(SBC_datasets(dat$variables[1:250,],
                        dat$generated[1:250]), 
                    bck,
                    cache_mode = "results",
                    cache_location = file.path(cache_dir, sprintf("res_%s", code, i)))
  write_csv(res$stats, file.path(cache_dir, sprintf("df_res_%s.rds", code)))
  write_csv(res$backend_diagnostics, file.path(cache_dir, sprintf("df_div_%s.rds", code)))
} else {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, sprintf("df_res_%s.rds", code)))
  df.backend = readRDS(file.path(cache_dir, sprintf("df_div_%s.rds", code)))
}

```

Again, we start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat, na.rm = T)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05, and `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples. This suggests that this model performs well and we can continue with our checks by plotting the simulated values to perform prior predictive checks. 

```{r sbc_rt2, fig.height=8}

if (!(file.exists(file.path(cache_dir, sprintf("dvfakemat_%s", code))))) {
  # create a matrix out of generated data
  dvname = gsub(" ", "", gsub("[\\|~].*", "", f.pal)[1])
  dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
  for (i in 1:length(dat[['generated']])) {
    dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
  }
  saveRDS(dvfakemat, file.path(cache_dir, sprintf("dvfakemat_%s", code)))
} else {
  dvfakemat = readRDS(file.path(cache_dir, sprintf("dvfakemat_%s", code)))
}

# compute one histogram per simulated data-set 
dvfakematH = dvfakemat
dvfakematH[dvfakematH > 2000] = 2000
breaks = seq(0, max(dvfakematH, na.rm=T), length.out = 101) 
binwidth = breaks[2] - breaks[1]
histmat = matrix(NA, ncol = dim(dvfakemat)[2], nrow = length(breaks)-1) 
for (i in 1:dim(dvfakemat)[2]) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat = as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Distribution of simulated reaction times", y = "", x = "") +
  theme_bw()

tmpM = apply(dvfakemat, 2, mean) # mean 
tmpSD = apply(dvfakemat, 2, sd) 
p2 = ggplot() + 
  stat_bin(aes(x = tmpM), fill = c_dark)  + 
  labs(x = "Mean discrimination", title = "Means of simulated reaction times") +
  theme_bw()
p3 = ggplot() + 
  stat_bin(aes(x = tmpSD), fill = c_dark) + 
  labs(x = "SD discrimination", title = "SDs of simulated reaction times") +
  theme_bw()
p = ggarrange(p1, 
  ggarrange(p2, p3, ncol = 2, labels = c("B", "C")), 
  nrow = 2, labels = "A")
annotate_figure(p, top = text_grob("Prior predictive checks", face = "bold", size = 14))

```

Subfigure A shows the distribution of the simulated data with bluer bands being more likely than greyer bands. Subfigure A shows a distribution that fits our expectations about reaction times in a simple decision task. The distribution of the means and standard deviations in the simulated datasets also look good. We go ahead with these priors and check the results of the SBC. We only plot the results from the models that had no divergence issues.

```{r sbc_rt3, fig.height=9}

# get simulation numbers with issues
rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = mean(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank != rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters
df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id))
plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3)) +
  ggtitle("Computational faithfulness: empirical cumulative distribution function")
plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3)) +
  ggtitle("Computational faithfulness: rank histograms")
plot_sim_estimated(df.results.b, alpha = 0.5) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3)) +
  ggtitle("Computational faithfulness: simulated and estimated values")
plot_contraction(df.results.b, 
                      prior_sd = setNames(
                        c(as.numeric(
                          gsub(".*, (.+)\\).*", "\\1", 
                               priors[priors$class == "Intercept",]$prior)), 
                          rep(0.04, times = (length(unique(df.results.b$variable))-1))), 
                                          unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 5)) +
  ggtitle("Computational faithfulness: contraction and z-values")

```

Next, we check the ranks of the parameters. If the model is unbiased, these should be uniformly distributed (Schad, Betancourt and Vasishth, 2020). The sample empirical cumulative distribution function (ECDF) lies within the theoretical distribution (95%) and the rank histogram also shows ranks within the 95% expected range, although there are some small deviations. We judge this to be acceptable.

Then, we investigated the relationship between the simulated true parameters and the posterior estimates. Although there are individual values diverging from the expected pattern, most parameters were recovered successfully within an uncertainty interval of alpha = 0.05. 

Last, we explore the z-score and the posterior contraction of our population-level predictors. The z-score "determines the distance of the posterior mean from the true simulating parameter", while the posterior contraction "estimates how much prior uncertainty is reduced in the posterior estimation" (Schad, Betancourt and Vasisth, 2020). All of this looks good for this model. 

## Posterior predictive checks

As the next step, we fit the model, check whether there are divergence or rhat issues, and then check whether the chains have converged.

```{r postpc_rt, fig.height=24, message=T}

# fit the final model
set.seed(2468)
m.pal = brm(f.pal,
            df.pal, prior = priors,
            family = shifted_lognormal,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_pal",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.pal$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.pal) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.pal)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 6)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no pathological behaviour with E-BFMI, no divergent sample and no rhats that are higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc_rt2, fig.height=15}

# get posterior predictions
post.pred = posterior_predict(m.pal, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.pal, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none") + xlim(0, 1500)

# distributions of means compared to the real values per group or conditions
p2 = ppc_stat_grouped(df.pal$rt.cor, 
                      post.pred, 
                      df.pal$diagnosis) + 
  theme_bw() + theme(legend.position = "none")
p3 = ppc_stat_grouped(df.pal$rt.cor, 
                      post.pred, 
                      df.pal$expected) + 
  theme_bw() + theme(legend.position = "none")
p4 = ppc_stat_grouped(df.pal$rt.cor, 
                      post.pred, 
                      df.pal$phase) + 
  theme_bw() + theme(legend.position = "none")
p5 = ppc_stat_grouped(df.pal$rt.cor, 
                      post.pred, 
                      df.pal$difficulty) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, p3, p4, p5, 
          nrow = 5, ncol = 1, labels = "AUTO")
annotate_figure(p, top = text_grob("Posterior predictive checks: reaction times", 
                                   face = "bold", size = 14))

```

The simulated data based on the model fits well with the real data, although there are slight deviations specifically for the predictor expectedness. However, it is much better than for the full model, so we judge this to be acceptable.  

## Inferences

Now that we are convinced that we can trust our model, we have a look at its estimate and use the hypothesis function to assess our hypotheses and perform explorative tests. 

```{r inf_rt, fig.height=18}

# print a summary
summary(m.pal)

# get the estimates and compute group comparisons
df.m.pal = post.draws %>% 
  select(starts_with("b_")) %>%
  mutate(
    b_COMP         = - b_diagnosis1 - b_diagnosis2 - b_diagnosis3,
    b_postvolatile = - b_phase1 - b_phase2,
    b_difficult    = - b_difficulty1 - b_difficulty2,
    ASD            = b_Intercept + b_diagnosis2,
    BOTH           = b_Intercept + b_diagnosis3,
    ADHD           = b_Intercept + b_diagnosis1,
    COMP           = b_Intercept + b_COMP,
    ASD_expected   = b_Intercept + b_diagnosis2 + b_expected1 + 
      `b_diagnosis2:expected1`,
    ASD_unexpected = b_Intercept + b_diagnosis2 - b_expected1 - 
      `b_diagnosis2:expected1`,
    ADHD_expected   = b_Intercept + b_diagnosis1 + b_expected1 + 
      `b_diagnosis1:expected1`,
    ADHD_unexpected = b_Intercept + b_diagnosis1 - b_expected1 - 
      `b_diagnosis1:expected1`,
    BOTH_expected   = b_Intercept + b_diagnosis3 + b_expected1 + 
      `b_diagnosis3:expected1`,
    BOTH_unexpected = b_Intercept + b_diagnosis3 - b_expected1 - 
      `b_diagnosis3:expected1`,
    COMP_expected   = b_Intercept - b_diagnosis1 - b_diagnosis2 - b_diagnosis3 + b_expected1 - 
      `b_diagnosis1:expected1` - `b_diagnosis2:expected1` - `b_diagnosis3:expected1`,
    COMP_unexpected   = b_Intercept - b_diagnosis1 - b_diagnosis2 - b_diagnosis3 - 
      b_expected1 + `b_diagnosis1:expected1` + `b_diagnosis2:expected1` - `b_diagnosis3:expected1`,
    `ADHD-ASD`     = ADHD - ASD,
    `ADHD-COMP`    = ADHD - COMP,
    `ASD-COMP`     = ASD  - COMP,
    h1b = (COMP_unexpected - COMP_expected) - (ASD_unexpected  - ASD_expected),
    h1c = (COMP_unexpected - COMP_expected) - (ADHD_unexpected - ADHD_expected),
    h1d = `b_diagnosis1:phase2` + 2*`b_diagnosis2:phase2`
  )

# plot the posterior distributions
df.m.pal %>%
  pivot_longer(cols = starts_with("b_"), names_to = "coef", values_to = "estimate") %>%
  subset(!startsWith(coef, "b_Int")) %>%
  mutate(
    coef = substr(coef, 3, nchar(coef)),
    coef = str_replace_all(coef, ":", " x "),
    coef = str_replace_all(coef, "diagnosis1", "ADHD"),
    coef = str_replace_all(coef, "diagnosis2", "ASD"),
    coef = str_replace_all(coef, "diagnosis3", "ADHD+ASD"),
    coef = str_replace_all(coef, "expected1", "expected"),
    coef = str_replace_all(coef, "expected2", "unexpected"),
    coef = str_replace_all(coef, "phase1", "prevolatile"),
    coef = str_replace_all(coef, "phase2", "volatile"),
    coef = str_replace_all(coef, "difficulty1", "easy"),
    coef = str_replace_all(coef, "difficulty2", "medium"),
    coef = fct_reorder(coef, desc(estimate))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>%
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(c_dark, c_light)) + theme(legend.position = "none")

## [!CHECK THOSE AGAIN!!!!

# H1b: COMP(unexp-exp) > ASD(unexp-exp)
h1b = hypothesis(m.pal, "0 < diagnosis1:expected1 + 2*diagnosis2:expected1")
h1b

# H1c: COMP(unexp-exp) != ADHD(unexp-exp)
h1c = hypothesis(m.pal, "0 > 2*diagnosis1:expected1 + diagnosis2:expected1", 
                 alpha = 0.025)
h1c

# H1d:COMP(volatile-prevolatile)) < ASD(volatile-prevolatile)
h1d = hypothesis(m.pal, "0 > -diagnosis1:phase1 - 2*diagnosis2:phase1 +
                 diagnosis1:phase2 + 2*diagnosis2:phase2")
h1d

# equivalence of the hypotheses
equ = equivalence_test(df.m.pal %>% select(h1b, h1c, h1d))
equ

## exploration: prevolatile phase

# prevolatile: COMP(unexpected - expected) != ASD(unexpected - expected)
e1.1 = hypothesis(m.pal, "0 < diagnosis1:expected1 + 
                  2*diagnosis2:expected1 +
                  diagnosis1:expected1:phase1 +
                  2*diagnosis2:expected1:phase1", alpha = 0.025)
e1.1

# prevolatile: COMP(unexpected - expected) != ADHD(unexpected - expected)
e1.2 = hypothesis(m.pal, "0 < 2*diagnosis1:expected1 + 
                  diagnosis2:expected1 +
                  2*diagnosis1:expected1:phase1 +
                  diagnosis2:expected1:phase1", alpha = 0.025)
e1.2

# prevolatile: ADHD(unexpected - expected) != ASD(unexpected - expected)
e1.3 = hypothesis(m.pal, "0 < -diagnosis1:expected1 + 
                  diagnosis2:expected1 -
                  diagnosis1:expected1:phase1 +
                  diagnosis2:expected1:phase1", alpha = 0.025)
e1.3

## exploration: task effects

# expected versus unexpected
e2.1 = hypothesis(m.pal, "0 > expected1", alpha = 0.025)
e2.1

# volatile versus prevolatile
e2.2 = hypothesis(m.pal, "0 < phase1 + 2*phase2", alpha = 0.025)
e2.2

# prevolatile versus postvolatile
e2.3 = hypothesis(m.pal, "0 < 2*phase1 + phase2", alpha = 0.025)
e2.3

# volatile versus postvolatile
e2.4 = hypothesis(m.pal, "0 < phase1 - phase2", alpha = 0.025)
e2.4

## extract predicted differences in ms instead of log data
df.new = df.pal %>% 
  select(diagnosis, phase, expected, difficulty) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, phase, expected, difficulty, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.pal, summary = F, 
               newdata = df.new %>% select(diagnosis, phase, expected, difficulty), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

# calculate our difference columns
df.ms = df.ms %>%
  mutate(
    COMP_expected     = rowMeans(across(matches("COMP_.*_expected_.*"))),
    COMP_unexpected   = rowMeans(across(matches("COMP_.*_unexpected_.*"))),
    ADHD_expected     = rowMeans(across(matches("ADHD_.*_expected_.*"))),
    ADHD_unexpected   = rowMeans(across(matches("ADHD_.*_unexpected_.*"))),
    ASD_expected      = rowMeans(across(matches("ASD_.*_expected_.*"))),
    ASD_unexpected    = rowMeans(across(matches("ASD_.*_unexpected_.*"))),
    COMP_unexp_exp    = COMP_unexpected - COMP_expected,
    ADHD_unexp_exp    = ADHD_unexpected - ADHD_expected,
    ASD_unexp_exp     = ASD_unexpected  - ASD_expected,
    h1b_COMPvASD_exp  = COMP_unexp_exp - ASD_unexp_exp,
    h1c_COMPvADHD_exp = COMP_unexp_exp - ADHD_unexp_exp,
    ASD_prevolatile   = rowMeans(across(matches("ASD_prevolatile_.*"))),
    ASD_volatile      = rowMeans(across(matches("ASD_volatile_.*"))),
    COMP_prevolatile  = rowMeans(across(matches("COMP_prevolatile_.*"))),
    COMP_volatile     = rowMeans(across(matches("COMP_volatile_.*"))),
    h1d_ASDvCOMP_volvpre = 
      (ASD_volatile - ASD_prevolatile) - (COMP_volatile - COMP_prevolatile),
    COMP_pre_unexp    = rowMeans(across(matches("COMP_prevolatile_unexpected_.*"))),
    COMP_pre_exp      = rowMeans(across(matches("COMP_prevolatile_expected_.*"))),
    ADHD_pre_unexp    = rowMeans(across(matches("ADHD_prevolatile_unexpected_.*"))),
    ADHD_pre_exp      = rowMeans(across(matches("ADHD_prevolatile_expected_.*"))),
    ASD_pre_unexp     = rowMeans(across(matches("ASD_prevolatile_unexpected_.*"))),
    ASD_pre_exp       = rowMeans(across(matches("ASD_prevolatile_expected_.*"))),
    e11_pre_COMPvASD_exp = 
      (COMP_pre_unexp - COMP_pre_exp) - (ASD_pre_unexp - ASD_pre_exp),
    e12_pre_COMPvADHD_exp = 
      (COMP_pre_unexp - COMP_pre_exp) - (ADHD_pre_unexp - ADHD_pre_exp),
    e13_pre_ADHDvASD_exp = 
      (ADHD_pre_unexp - ADHD_pre_exp) - (ASD_pre_unexp - ASD_pre_exp),
    e21_exp           = rowMeans(across(matches(".*_unexpected_.*"))) - 
      rowMeans(across(matches(".*_expected_.*"))),
    e22_volvpre       = rowMeans(across(matches(".*_volatile_.*"))) - 
      rowMeans(across(matches(".*_prevolatile_.*"))),
    e23_prevpost      = rowMeans(across(matches(".*_prevolatile_.*"))) - 
      rowMeans(across(matches(".*_postvolatile_.*"))),
    e24_volvpost      = rowMeans(across(matches(".*_volatile_.*"))) - 
      rowMeans(across(matches(".*_postvolatile_.*"))),
    e25_diffvmed      = rowMeans(across(matches(".*_difficult"))) - 
      rowMeans(across(matches(".*_medium"))),
    e25_medveasy      = rowMeans(across(matches(".*_medium"))) - 
      rowMeans(across(matches(".*_easy"))),
    e25_diffveasy     = rowMeans(across(matches(".*_difficult"))) - 
      rowMeans(across(matches(".*_easy")))
  )

lapply(df.ms %>% select(starts_with("e") | starts_with("h")), ci)

```

[!MISSING]

## Plots

```{r plot_rt, fig.height=16}

# rain cloud plot including all factors

df.pal %>%
  ggplot(aes(diagnosis, rt.cor, fill = expected, colour = expected)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show.legend = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  #ylim(0, 1) +
  labs(title = "Reaction times per subject", x = "", y = "rt (ms)") +
  facet_wrap(difficulty ~ phase) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", text = element_text(size = 15))
```

```{r plot_rt2, fig.height=6}

# rain cloud plot excluding difficulty

df.pal %>%
  group_by(subID, diagnosis, expected, phase) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  ) %>%
  ggplot(aes(diagnosis, rt.cor, fill = expected, colour = expected)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show.legend = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  #ylim(0, 1) +
  labs(title = "Reaction times per subject", x = "", y = "rt (ms)") +
  facet_wrap(. ~ phase) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", text = element_text(size = 15))


```

```{r plot_rt3, fig.height=6}

# two-way interactions
p1 = df.pal %>%
  group_by(subID, diagnosis, expected) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  ) %>% 
  group_by(diagnosis, expected) %>%
  summarise(
    rt.mn = mean(rt.cor),
    rt.se = sd(rt.cor) / sqrt(n())
  ) %>%
  ggplot(aes(y = rt.mn, x = diagnosis, group = expected, colour = expected)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = rt.mn - rt.se, 
                    ymax = rt.mn + rt.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "diagnosis", y = "rt (ms)") + 
  ggtitle("Diagnosis x expectancy") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))
p2 = df.pal %>%
  group_by(subID, diagnosis, phase) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  )  %>% 
  group_by(diagnosis, phase) %>%
  summarise(
    rt.mn = mean(rt.cor),
    rt.se = sd(rt.cor) / sqrt(n())
  ) %>%
  ggplot(aes(y = rt.mn, x = phase, group = diagnosis, colour = diagnosis)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = rt.mn - rt.se, 
                    ymax = rt.mn + rt.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "phase", y = "rt (ms)") + 
  ggtitle("Diagnosis x phase") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))

ggarrange(p1, p2, ncol = 2)

p3 = df.pal %>%
  group_by(subID, diagnosis, difficulty) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  )  %>% 
  group_by(diagnosis, difficulty) %>%
  summarise(
    rt.mn = mean(rt.cor),
    rt.se = sd(rt.cor) / sqrt(n())
  ) %>%
  ggplot(aes(y = rt.mn, x = difficulty, group = diagnosis, colour = diagnosis)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = rt.mn - rt.se, 
                    ymax = rt.mn + rt.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "difficulty", y = "rt (ms)") + 
  ggtitle("Diagnosis x difficulty") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))
p4 = df.pal %>%
  group_by(subID, phase, difficulty) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  )  %>% 
  group_by(phase, difficulty) %>%
  summarise(
    rt.mn = mean(rt.cor),
    rt.se = sd(rt.cor) / sqrt(n())
  ) %>%
  ggplot(aes(y = rt.mn, x = phase, group = difficulty, colour = difficulty)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = rt.mn - rt.se, 
                    ymax = rt.mn + rt.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "phase", y = "rt (ms)") + 
  ggtitle("Phase x difficulty") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))

ggarrange(p3, p4, ncol = 2)

p5 = df.pal %>%
  group_by(subID, expected, phase) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  )  %>% 
  group_by(phase, expected) %>%
  summarise(
    rt.mn = mean(rt.cor),
    rt.se = sd(rt.cor) / sqrt(n())
  ) %>%
  ggplot(aes(y = rt.mn, x = phase, group = expected, colour = expected)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = rt.mn - rt.se, 
                    ymax = rt.mn + rt.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "phase", y = "rt (ms)") + 
  ggtitle("Phase x expectancy") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))
p6 = df.pal %>%
  group_by(subID, expected, difficulty) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  )  %>% 
  group_by(expected, difficulty) %>%
  summarise(
    rt.mn = mean(rt.cor),
    rt.se = sd(rt.cor) / sqrt(n())
  ) %>%
  ggplot(aes(y = rt.mn, x = difficulty, group = expected, colour = expected)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = rt.mn - rt.se, 
                    ymax = rt.mn + rt.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "difficulty", y = "rt (ms)") + 
  ggtitle("Expectancy x difficulty") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))

ggarrange(p5, p6, ncol = 2)

# three-way interaction
df.pal %>%
  group_by(subID, diagnosis, expected, phase) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  )  %>% 
  group_by(diagnosis, expected, phase) %>%
  summarise(
    rt.mn = mean(rt.cor),
    rt.se = sd(rt.cor) / sqrt(n())
  ) %>%
  ggplot(aes(y = rt.mn, x = expected, group = diagnosis, colour = diagnosis)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = rt.mn - rt.se, 
                    ymax = rt.mn + rt.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "expectancy", y = "rt (ms)") + 
  ggtitle("Diagnosis x phase x expectancy") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  facet_wrap(. ~ phase) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))
df.pal %>%
  group_by(subID, diagnosis, expected, difficulty) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  )  %>% 
  group_by(diagnosis, expected, difficulty) %>%
  summarise(
    rt.mn = mean(rt.cor),
    rt.se = sd(rt.cor) / sqrt(n())
  ) %>%
  ggplot(aes(y = rt.mn, x = diagnosis, group = expected, colour = expected)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = rt.mn - rt.se, 
                    ymax = rt.mn + rt.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "diagnosis", y = "rt (ms)") + 
  ggtitle("Diagnosis x difficulty x expectancy") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  facet_wrap(. ~ difficulty) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))
df.pal %>%
  group_by(subID, diagnosis, phase, difficulty) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  )  %>% 
  group_by(diagnosis, difficulty, phase) %>%
  summarise(
    rt.mn = mean(rt.cor),
    rt.se = sd(rt.cor) / sqrt(n())
  ) %>%
  ggplot(aes(y = rt.mn, x = difficulty, group = diagnosis, colour = diagnosis)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = rt.mn - rt.se, 
                    ymax = rt.mn + rt.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "difficulty", y = "rt (ms)") + 
  ggtitle("Diagnosis x phase x difficulty") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  facet_wrap(. ~ phase) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))
df.pal %>%
  group_by(subID, expected, phase, difficulty) %>%
  summarise(
    rt.cor = median(rt.cor, na.rm = T)
  )  %>% 
  group_by(expected, phase, difficulty) %>%
  summarise(
    rt.mn = mean(rt.cor),
    rt.se = sd(rt.cor) / sqrt(n())
  ) %>%
  ggplot(aes(y = rt.mn, x = phase, group = expected, colour = expected)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = rt.mn - rt.se, 
                    ymax = rt.mn + rt.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  labs (x = "phase", y = "rt (ms)") + 
  ggtitle("Phase x expectancy x difficulty") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  facet_wrap(. ~ difficulty) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))

```

## Bayes factor analysis

```{r bf_rt, fig.height=8}

# set the directory in which to save results
sense_dir = file.path(getwd(), "_brms_sens_cache")
main.code = "pal_rt"

# rerun the model with more iterations for bridgesampling
set.seed(5544)
m.pal.bf = brm(f.pal,
          df.pal, prior = priors,
          family = shifted_lognormal,
          iter = 40000, warmup = 10000,
          backend = "cmdstanr", threads = threading(8),
          file = "m_pal_bf", silent = 2,
          save_pars = save_pars(all = TRUE)
          )

# first, run it only for the chosen parameters to determine the best models
tryCatch({
  # check if it has been run before
  if (!file.exists(file.path(sense_dir, 
                             sprintf("df_%s_bf.csv", main.code)))) {
    # use function to compute BF with the described priors
    bf_sens_4int(m.pal.bf, "diagnosis", "expected", "phase", "difficulty", 
                 "chosen", 
                 main.code, # prefix for all models and MLL
                 file.path("logfiles", "log_PAL_bf.txt"), # log file
                 sense_dir # where to save the models and MLL
    )
  }
  },
  error = function(err) {
    message(sprintf("Error for %s: %s", "chosen", err))
  }
)

# since there are 167 models in total, we focus on the 20 best models for the 
# sensitivity analysis
df.pal.bf = read_csv(file.path(sense_dir, 
                               sprintf("df_%s_bf.csv", main.code)), 
                     show_col_types = F) %>% 
  ungroup() %>% 
  filter(priors == "chosen") %>% 
  mutate(bf.rank = rank(bf.log), 
         bf.rank = max(bf.rank) - bf.rank + 1
         ) %>%
  filter(bf.rank <= 20)
fixed = df.pal.bf$`population-level`

# and describe the priors we want to use in our sensitivity analysis
pr.descriptions = c(
  "sdx2",    "sdx4",   "sdx8", 
  "sdx0.5", "sdx0.25", "sdx0.125"
  )

# check if they have been run already
pr.done = read_csv(file.path(sense_dir, 
                               sprintf("df_%s_bf.csv", main.code)), 
                     show_col_types = F) %>% 
    select(priors) %>% distinct()
pr.descriptions = pr.descriptions[!(pr.descriptions %in% pr.done$priors)]

# loop through the priors that have not been used before
for (pr.desc in pr.descriptions) {
  tryCatch({
    # use function to compute BF with the described priors
    bf_sens_fixed(m.pal.bf, fixed, 
                 pr.desc, 
                 main.code, # prefix for all models and MLL
                 file.path("logfiles", "log_PAL_bf.txt"), # log file
                 sense_dir # where to save the models and MLL
    )
  },
  error = function(err) {
    message(sprintf("Error for %s: %s", pr.desc, err))
  }
  )
}

df.pal.bf = read_csv(file.path(sense_dir,
                               sprintf("df_%s_bf.csv", main.code)),
                     show_col_types = F)

# check the sensitivity analysis result per model
df.pal.bf %>%
  filter(`population-level` != "1") %>%
  mutate(
    sd = as.factor(case_when(
      priors == "chosen" ~ "1",
      substr(priors, 1, 3) == "sdx" ~ gsub("sdx", "", priors),
      T ~ priors)
    ),
    order = case_when(
      priors == "chosen" ~ 1,
      substr(priors, 1, 3) == "sdx" ~ as.numeric(gsub("sdx", "", priors)),
      T ~ 999),
    sd = fct_reorder(sd, order)
  ) %>%
  ggplot(aes(y = bf.log,
             x = sd,
             group = `population-level`,
             colour = `population-level`)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = "1") +
  geom_hline(yintercept = 0) +
  ggtitle("Sensitivity analysis with the intercept-only model as reference") +
  #scale_colour_manual(values = custom.col) +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# There seems to be a bit of a change between the best few models from the chosen
# prior to wider priors, let's inspect this more closely
df.pal.bf.best = df.pal.bf %>% 
  # choose the best models based on the chosen priors
  filter(priors == "chosen") %>%
  mutate(
    bf.rank = max(rank(bf.log)) - rank(bf.log) + 1
  ) %>% 
  filter(bf.rank < 5) %>%
  # add a model number to make plotting easier
  mutate(
    model.nr = paste("model", bf.rank)
  ) %>%
  select(`population-level`, model.nr) %>%
  # merge with the other data again
  merge(., df.pal.bf)

#kable(df.pal.bf.best %>%
#  select(-bf.rank) %>%
#  pivot_wider(names_from = priors, values_from = bf.log) %>% arrange(desc(chosen)))

df.pal.bf.best %>%
  mutate(
    sd = as.factor(case_when(
      priors == "chosen" ~ "1",
      substr(priors, 1, 3) == "sdx" ~ gsub("sdx", "", priors),
      T ~ priors)
    ),
    order = case_when(
      priors == "chosen" ~ 1,
      substr(priors, 1, 3) == "sdx" ~ as.numeric(gsub("sdx", "", priors)),
      T ~ 999),
    sd = fct_reorder(sd, order)
  ) %>%
  ggplot(aes(y = bf.log,
             x = sd,
             group = model.nr ,
             colour = model.nr)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = "1") +
  ggtitle("Sensitivity analysis with the intercept-only model as reference") +
  theme_bw() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))


```

# S1.4 Exploration of reaction times in all non-outlier trials

Since other studies have also used all non-outlier trials regardless of accuracy, we compute the above described model on these RTs as well. 

```{r rt_use, fig.height=9, message=T}

# fit the final model
set.seed(2468)
m.use = brm(rt.use ~ diagnosis * expected * phase * difficulty +
                   (expected + phase + difficulty +
                      expected:phase + difficulty:phase + expected:difficulty | subID),
            df.pal, prior = priors,
            family = shifted_lognormal,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_pal_rtuse",
            save_pars = save_pars(all = TRUE)
            )
rstan::check_hmc_diagnostics(m.use$fit)

# check that rhats are below 1.01
sum(brms::rhat(m.use) >= 1.01, na.rm = T)

# check the trace plots
post.draws = as_draws_df(m.use)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 6)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

# print the model summary
summary(m.use)

# compare the fixed effects between both models
effects = as.data.frame(fixef(m.pal)) %>%
  mutate(model = "cor") %>% 
  rownames_to_column(., var = "fixed effect") %>%
  merge(., as.data.frame(fixef(m.use)) %>%
  mutate(model = "use") %>% 
  rownames_to_column(., var = "fixed effect"), 
        all = T) %>%
  select(model, `fixed effect`, Estimate) %>%
  pivot_wider(names_from = model, values_from = c(Estimate)) %>%
  mutate(
    Estimate.diff = cor - use
  )

kable(effects)
  
```

# S1.4 Exploration of association type

Now, we explore if there were any differences between the tone-valence associations at the beginning. 

```{r tone_pic}

# check normal distribution
df.tp %>% group_by(diagnosis, tone_pic, expected) %>%
  shapiro_test(rt.cor) %>%
    mutate(
      sig = if_else(p < 0.05, "*", "")
    )

# rank transform the data
df.tp = df.tp %>%
  ungroup() %>%
  mutate(
    rrt.cor   = rank(rt.cor),
    tone_pic  = as.factor(tone_pic)
    )

# run the ANOVA
aov.tp = anovaBF(rrt.cor   ~ diagnosis * tone_pic * expected, data = df.tp)
bf.tp  = extractBF(aov.tp, logbf = T)
kable(head(bf.tp %>% arrange(desc(bf.tp))))
  

```

The best model contains all three main effects suggesting that while certain combinations were associated with faster and others with slower reaction times, this was independent of the diagnostic status and expectancy. 

# S1.5 Explorative analysis of errors

Last but not least, we are going to explore possible differences with regards to mean accuracies using a Bayesian ANOVA. [!Switch to something better????]

```{r acc}

# check normal distribution
df.pal %>% group_by(diagnosis, phase, expected, difficulty) %>%
  shapiro_test(acc) %>%
    mutate(
      sig = if_else(p < 0.05, "*", "")
    )

# rank transform the data
df.acc = df.pal %>%
  ungroup() %>%
  mutate(
    racc   = rank(acc)
    )

# run the ANOVA
if (!file.exists("aov_acc.rds")){
  aov.acc   = anovaBF(racc   ~ diagnosis * phase * expected * difficulty, data = df.acc)
  saveRDS(aov.acc, file = "aov_acc.rds")
} else {
  aov.acc = readRDS("aov_acc.rds")
}

bf.acc = extractBF(aov.acc, logbf = T)
kable(head(bf.acc %>% arrange(desc(bf))))

# print overall accuracy rates for all the effects included in the best model
df.acc.diagnosis = df.acc %>% 
  group_by(diagnosis) %>% 
  summarise(mean_accuracy = mean(acc, na.rm = T), 
            sd_accuracy = sd(acc, na.rm = T))
df.acc.diagnosis

df.acc %>% 
  group_by(expected) %>% 
  summarise(mean_accuracy = mean(acc, na.rm = T), 
            sd_accuracy = sd(acc, na.rm = T))

df.acc %>% 
  group_by(difficulty) %>% 
  summarise(mean_accuracy = mean(acc, na.rm = T), 
            sd_accuracy = sd(acc, na.rm = T))

df.acc %>% 
  group_by(expected, difficulty) %>% 
  summarise(mean_accuracy = mean(acc, na.rm = T), 
            sd_accuracy = sd(acc, na.rm = T))

```

Accuracies were generally high, with a grand average of `r round(mean(df.acc.diagnosis$mean_accuracy),2)`% accurate responses across diagnostic groups. Accuracies seems to have differed between diagnostic groups. Let's do some plotting to figure out where our differences lie. 

## Plots

```{r plot_acc, fig.height=12}

# rain cloud plot including all factors

df.acc %>%
  ggplot(aes(expected, acc, fill = diagnosis, colour = diagnosis)) + #
  geom_rain(rain.side = 'r',
boxplot.args = list(color = "black", outlier.shape = NA, show.legend = FALSE, alpha = .8),
violin.args = list(color = "black", outlier.shape = NA, alpha = .8),
boxplot.args.pos = list(
  position = ggpp::position_dodgenudge(x = 0, width = 0.3), width = 0.3
),
point.args = list(show_guide = FALSE, alpha = .5),
violin.args.pos = list(
  width = 0.6, position = position_nudge(x = 0.16)),
point.args.pos = list(position = ggpp::position_dodgenudge(x = -0.25, width = 0.1))) +
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  labs(title = "Accuracies per subject", x = "", y = "accuracy (%)") +
  facet_wrap(phase ~ difficulty) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", text = element_text(size = 15))

```

```{r plot_acc2, fig.height=4}

# plot the two-way interaction
df.acc %>%
  group_by(subID, expected, difficulty) %>%
  summarise(
    acc = mean(acc, na.rm = T)
  ) %>% 
  group_by(expected, difficulty) %>%
  summarise(
    acc.mn = mean(acc),
    acc.se = sd(acc) / sqrt(n())
  ) %>%
  ggplot(aes(y = acc.mn, x = difficulty, group = expected, colour = expected)) +
  geom_line(position = position_dodge(0.4), linewidth = 1) +
  geom_errorbar(aes(ymin = acc.mn - acc.se, 
                    ymax = acc.mn + acc.se), linewidth = 1, width = 0.5, 
                position = position_dodge(0.4)) + 
  geom_point(position = position_dodge(0.4), size = 5) + 
  ylim(75,100) +
  labs (x = "difficulty", y = "accuracy") + 
  ggtitle("Difficulty x expectancy") + 
  scale_fill_manual(values = custom.col) +
  scale_color_manual(values = custom.col) +
  theme_bw() + 
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))

```